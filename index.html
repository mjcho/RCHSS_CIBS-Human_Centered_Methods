<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="卓牧融 | Mu-Jung ‘MJ’ Cho">
  <meta name="dcterms.date" content="2025-11-24">
  <title>Exploring Holistic Human-Centered Empirical Methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto-45b65b5700542dd985ac633cd867b815.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExNXBnenU0Y2I2NmY5azB4eDM1cml4bGo0MmgxMjN4eWYxazc5dTZscSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/sJvz8Qnfly3BOuotGx/giphy.gif" data-background-opacity="0.2" class="quarto-title-block center">
  <h1 class="title">Exploring Holistic Human-Centered Empirical Methods</h1>
  <p class="subtitle">Case Studies in Multimodal Research</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
卓牧融 | Mu-Jung ‘MJ’ Cho <a href="https://orcid.org/0000-0001-5500-6336" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
</div>
<div class="quarto-title-author-email">
<a href="mailto:mjcho@as.edu.tw">mjcho@as.edu.tw</a>
</div>
        <p class="quarto-title-affiliation">
            RCHSS, Academia Sinica
          </p>
    </div>
</div>

  <p class="date">2025-11-24</p>
</section>
<section>
<section id="screenomics-3.0-a-framework-for-visual-digital-trace-research" class="title-slide slide level1 center">
<h1>Screenomics 3.0: A Framework for Visual Digital Trace Research</h1>
<p>@ 中央研究院 |&gt; TIGP<br>
@ TIGP |&gt; Academia Sinica<br>
2025.11.21</p>
</section>
<section class="slide level2">

<!-- ## Visual Digital Trace Research with Smartphone Screens -->
</section>
<section id="why-study-real-world-digital-interactions" class="slide level2">
<h2>Why Study Real-World Digital Interactions?</h2>
<video data-src="vids/SarahClip40s_Slow.mp4" class="r-stretch" controls=""><a href="vids/SarahClip40s_Slow.mp4">Video</a></video>
</section>
<section id="characteristics-of-real-world-interactions" class="slide level2">
<h2>Characteristics of Real-World Interactions</h2>
<ul>
<li class="fragment">Increasing reliance on <span class="neon-green">digital media</span>.<br>
</li>
<li class="fragment">Interactions are <span class="neon-green">rapid and bursty</span> across platforms.<br>
</li>
<li class="fragment"><span class="neon-green">Fragmentation</span> of content categories.<br>
</li>
<li class="fragment"><span class="neon-green">Time domain</span> issues: exposure over short vs.&nbsp;long intervals.<br>
</li>
<li class="fragment"><span class="neon-green">Idiosyncrasy</span> across individuals.</li>
</ul>
<p>All of these challenge <span class="neon-cyan">conventional social and behavioral research methods</span>.</p>
</section>
<section id="challenges-in-capturing-digital-trace-data" class="slide level2" data-background-image="imgs/nilam.jpg" data-background-opacity="0.2">
<h2>Challenges in Capturing Digital Trace Data</h2>
<h3 id="screens-as-digital-trace-data-dtd">Screens as Digital Trace Data (DTD)</h3>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li class="fragment">DTD: “records of activity (trace data) undertaken through an online information system (thus, digital).” <span class="citation">(Howison et al., 2011).</span><br>
</li>
<li class="fragment">Screens vs.&nbsp;Platform APIs &amp; data donation:
<ul>
<li class="fragment">Platform-specific vs.&nbsp;<strong>user-specific</strong> <span class="citation">(Ohme et al., 2024).</span></li>
<li class="fragment">Capture a <strong>broader spectrum</strong> of interactions.<br>
</li>
<li class="fragment"><strong>Multimodality</strong>: images, text, interface elements, mixed content.<br>
</li>
<li class="fragment">Flexible <strong>unit of analysis</strong> (screen, session, episode).<br>
</li>
<li class="fragment">Ease of <strong>passive data collection</strong>.</li>
</ul></li>
</ul>
</div><div class="column" style="width:30%;">
<p><img data-src="imgs/nilam.jpg" style="width:100.0%"></p>
</div></div>
</section>
<section id="the-stanford-human-screenome-project" class="slide level2">
<h2>The Stanford Human Screenome Project</h2>
<h3 id="screenome-capturing-real-world-interactions">Screenome: Capturing Real-World Interactions</h3>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li class="fragment">Captures smartphone screens every <strong>5 seconds (or less)</strong>.<br>
</li>
<li class="fragment">~<strong>500 million screens</strong> from over <strong>1,000 people</strong> for up to <strong>1 year</strong>.<br>
</li>
<li class="fragment">Privacy, risk, and <strong>data security</strong> considerations.<br>
</li>
<li class="fragment">Linkage to periodic <strong>surveys (health data)</strong>.</li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="imgs/nature.jpg" style="width:100.0%"> <span class="citation">Reeves et al.&nbsp;(2020). <em>Nature</em>; Reeves et al.&nbsp;(2020). <em>Human–Computer Interaction</em>.</span></p>
</div></div>
</section>
<section id="expanding-on-the-screenome-approach" class="slide level2 scrollable">
<h2>Expanding on the Screenome Approach</h2>
<h3 id="transition-to-screenomics-3.0">Transition to Screenomics 3.0</h3>
<div class="columns" width="60%">
<div class="column">
<ol type="1">
<li class="fragment"><strong>Screenome 1.0</strong> – Research infrastructure &amp; conventional ML-based measurements.<br>
</li>
<li class="fragment"><strong>Screenome 2.0</strong> – Deep learning-based content analysis.<br>
</li>
<li class="fragment"><strong>Screenomics 3.0</strong> – Multimodal encoders and large multimodal models (LMMs).</li>
</ol>
</div><div class="column" style="width:40%;">
<p><img data-src="imgs/scr1p0.png" style="width:100.0%"> <span class="citation">Cho et al.&nbsp;(in prep.).</span></p>
</div></div>
</section>
<section id="ml-based-content-analysis-mixed-methods" class="slide level2">
<h2>ML-Based Content Analysis &amp; Mixed Methods</h2>
<h3 id="media-sequencing-and-family-dynamics">Media Sequencing and Family Dynamics</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Homeostatic mechanism of <strong>media sequencing</strong> in everyday life.<br>
</li>
<li class="fragment">Young adults’ smartphone interactions with <strong>family</strong>.<br>
</li>
<li class="fragment">ML-based content analysis combined with <strong>qualitative interviews</strong> and diary methods.<br>
</li>
<li class="fragment">Focus on how digital interactions <strong>sequenced with offline interactions</strong> shape family dynamics.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/homeostasis.png" style="width:100.0%"> <span class="citation">Cho et al.&nbsp;(2023). <em>Heliyon</em>.</span> <img data-src="imgs/ya.png" style="width:100.0%"> <span class="citation">Sun et al.&nbsp;(2023). Journal of Social and Personal Relationships.</span></p>
</div></div>
</section>
<section id="ml-based-content-analysis-mixed-methods-1" class="slide level2">
<h2>ML-Based Content Analysis &amp; Mixed Methods</h2>
<h3 id="mental-health-via-screenome">Mental Health via Screenome</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Linking <strong>survey measures</strong> with <strong>digital trace data (DTD)</strong>:
<ul>
<li class="fragment">Depression, State Anxiety, ADHD, Happiness.<br>
</li>
<li class="fragment">Integrating <strong>self-report scales</strong> with DTD.<br>
</li>
<li class="fragment"><strong>Real-time, personalized detection</strong> of mental health states.<br>
</li>
<li class="fragment">Complementing <strong>clinical and survey</strong> indicators.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/mental_health.png" style="width:100.0%"> <span class="citation">Cerit et al.&nbsp;(2025). <em>JMIR Formative Research</em>.</span></p>
</div></div>
</section>
<section id="deep-learning-based-content-analysis" class="slide level2">
<h2>Deep Learning-Based Content Analysis</h2>
<h3 id="visual-emotions-scene-recognition-food-detection">Visual Emotions, Scene Recognition, Food Detection</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">CNN-based <strong>scene recognition</strong> to categorize environments.<br>
</li>
<li class="fragment">CNN-based <strong>food detection</strong> for identifying food-related content.<br>
</li>
<li class="fragment">Visual <strong>emotion recognition</strong> (valence and arousal) from screen images.<br>
</li>
<li class="fragment">Weekly trends: link <strong>visual emotion trajectories</strong> to well-being.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/valence.png" style="width:100.0%"> <img data-src="imgs/scene.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="multimodal-encoders-and-lmms" class="slide level2">
<h2>Multimodal Encoders and LMMs</h2>
<h3 id="adolescents-food-related-content-exposure">Adolescents’ Food-Related Content Exposure</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>20 adolescents</strong>, 1-week observation of smartphone screens.<br>
</li>
<li class="fragment"><strong>3%</strong> of all exposure is food-related; <strong>0.6%</strong> branded.<br>
</li>
<li class="fragment">Demonstrates how a specific <strong>content category</strong> can be mined from large screen sets.<br>
</li>
<li class="fragment">Shows feasibility of <strong>longitudinal content tracking</strong> at screen level.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/food_adol.png" style="width:100.0%"> <span class="citation">Cho et al.&nbsp;(in prep.).</span></p>
</div></div>
</section>
<section id="multimodal-encoders-and-lmms-1" class="slide level2">
<h2>Multimodal Encoders and LMMs</h2>
<h3 id="spatial-behavioral-clusters-with-clip-ocr">Spatial + Behavioral Clusters with CLIP &amp; OCR</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>Screenotype</strong>: unique screenomes associated with behaviors and experiences.<br>
</li>
<li class="fragment">Used <strong>HDBSCAN + UMAP</strong> on 320K data points for cluster analysis.<br>
</li>
<li class="fragment"><strong>26 distinct clusters</strong>; ~19% non-noise.<br>
</li>
<li class="fragment">Within-app <strong>CLIP variance &gt; between-app variance</strong> → rich within-app heterogeneity.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/screenotype.png" style="width:100.0%"> <span class="citation">Cho et al.&nbsp;(in prep.).</span></p>
</div></div>
</section>
<section id="multimodal-encoders-and-lmms-2" class="slide level2" data-background-image="imgs/atlas.png" data-background-opacity="0.2">
<h2>Multimodal Encoders and LMMs</h2>
<h3 id="mapping-digital-screen-content">Mapping Digital Screen Content</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>Media Content Atlas</strong> for open-ended exploration of digital media interactions.<br>
</li>
<li class="fragment">Content mapping with <strong>HDBSCAN + UMAP</strong> on <strong>1.12M</strong> data points from <strong>112 participants</strong>.<br>
</li>
<li class="fragment">Supports hypothesis generation about <strong>media repertoires</strong> and <strong>use patterns</strong>.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/atlas.png" style="width:100.0%"> <span class="citation">Cerit et al.&nbsp;(2025). CHI EA.</span></p>
</div></div>
</section>
<section id="multimodal-encoders-and-lmms-3" class="slide level2" data-background-image="imgs/atlas_infra.png" data-background-opacity="0.1">
<h2>Multimodal Encoders and LMMs</h2>
<h3 id="mapping-digital-screen-content-1">Mapping Digital Screen Content</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>Image + text embeddings</strong> combined.
<ul>
<li class="fragment">Large multimodal model (LMM) descriptions.<br>
</li>
</ul></li>
<li class="fragment"><strong>Topic label generation</strong> for clusters of screens.<br>
</li>
<li class="fragment"><strong>Information retrieval</strong>: querying screens and descriptions for:
<ul>
<li class="fragment">Content categories.</li>
<li class="fragment">usage contexts, etc.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/atlas_infra.png" style="width:100.0%"> <span class="citation">Cerit et al.&nbsp;(2025). CHI EA.</span></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="overall-pipeline">Overall Pipeline</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>Screenomes</strong>: longitudinal smartphone screenshots.<br>
</li>
<li class="fragment"><strong>Image Encoder</strong> (CLIP, EVA-CLIP, e5-V) → <strong>image embeddings</strong>.<br>
</li>
<li class="fragment"><strong>OCR Engine</strong> (e.g., CRAFT + STR) → text from screens.<br>
</li>
<li class="fragment"><strong>Text Encoder</strong> (MiniLM, distilRoBERTa) → <strong>document embeddings</strong>.<br>
</li>
<li class="fragment"><strong>Task-specific labels</strong> with LMM + PEFT (Gemma 3 + LoRA).<br>
</li>
<li class="fragment">Users provide <strong>human labels</strong>; high-power LMMs supply <strong>synthetic labels</strong>.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/scr3_arch.png" style="width:100.0%"> <span class="citation">Cho et al.&nbsp;(in prep.).</span></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture-1" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="image-encoder-clip">Image Encoder: CLIP</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">CLIP (Contrastive Language–Image Pre-training).<br>
</li>
<li class="fragment">Jointly learns <strong>image–text representations</strong>.<br>
</li>
<li class="fragment">Surpasses many fully supervised baselines on zero-shot tasks.<br>
</li>
<li class="fragment">Core to many <strong>state-of-the-art multimodal architectures</strong>.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/clip.png" style="width:100.0%"> <span class="citation">Radford et al.&nbsp;(2021).</span></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture-2" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="image-encoder-eva-clip">Image Encoder: EVA-CLIP</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">EVA (Explore the limits of Visual representation at scAle).
<ul>
<li class="fragment">Pretrains <strong>ViTs</strong> using masked image modeling (MIM).<br>
</li>
<li class="fragment">Helps ViTs learn fine-grained image structure before contrastive learning.<br>
</li>
</ul></li>
<li class="fragment">EVA-02-CLIP: outperforms similar-sized models on many tasks, including <strong>image retrieval</strong>.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/eva_clip.png" style="width:100.0%"> <span class="citation">Fang et al.&nbsp;(2023).</span></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture-3" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="text-to-image-retrieval-with-clip">Text-to-Image Retrieval with CLIP</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Compute <strong>cosine similarity</strong> between text queries and image embeddings.<br>
</li>
<li class="fragment">Efficiently find <strong>relevant (and irrelevant)</strong> images.<br>
</li>
<li class="fragment">Supports rapid creation of <strong>balanced labeled datasets</strong>.<br>
</li>
<li class="fragment">Enables exploratory analyses of <strong>usage contexts</strong> and <strong>media patterns</strong>.</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/clip_bev.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture-4" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="human-labels-with-label-studio">Human Labels with Label Studio</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>Label Studio</strong> for customized labeling interfaces across projects.<br>
</li>
<li class="fragment">Supports <strong>multiple annotation tasks</strong> (e.g., food presence, valence, task type).<br>
</li>
<li class="fragment">Reliability is a key challenge:
<ul>
<li class="fragment">need for consistent instructions,<br>
</li>
<li class="fragment">quality control and adjudication.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/human_label.png" style="width:100.0%"> <img data-src="imgs/food_irr.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture-5" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="task-specific-labels-with-large-multimodal-models">Task-Specific Labels with Large Multimodal Models</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>LLaVA</strong> (Large Language and Vision Assistant): open-source LMM.<br>
</li>
<li class="fragment">Use cases in this project:
<ul>
<li class="fragment"><strong>VQA</strong> over smartphone screens,<br>
</li>
<li class="fragment"><strong>PEFT</strong> for task-specific labeling,<br>
</li>
<li class="fragment">Large-N inference over millions of screens.<br>
</li>
</ul></li>
<li class="fragment">Competitive families of models:
<ul>
<li class="fragment">Gemma 3 (12B, 8-bit quantization).<br>
</li>
<li class="fragment">Qwen2.5-VL (7B, bf16).<br>
</li>
<li class="fragment">Llama 3.2-V (11B, 8-bit, split across GPUs).</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/task_specific_label.png" style="width:100.0%"> <span class="citation">Liu et al.&nbsp;(2023).</span></p>
</div></div>
</section>
<section id="screenomics-3.0-architecture-6" class="slide level2">
<h2>Screenomics 3.0 Architecture</h2>
<h3 id="peft-with-lora">PEFT with LoRA</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">LoRA (Low-Rank Adaptation of Large Language Models).
<ul>
<li class="fragment">Replace full-rank weight matrix <strong>W (d × d)</strong> with low-rank <strong>A (d × r)</strong> and <strong>B (r × d)</strong>.<br>
</li>
<li class="fragment">Freeze <strong>W</strong>, train only <strong>B · A</strong>.<br>
</li>
</ul></li>
<li class="fragment">Makes large-model fine-tuning <strong>feasible</strong> on modest hardware.
<ul>
<li class="fragment">Pretraining → SFT (supervised fine-tuning) → RLHF.<br>
</li>
<li class="fragment">Reduces trainable parameters and memory footprint substantially.<br>
</li>
</ul></li>
<li class="fragment">Example performance (food-detection use case):
<ul>
<li class="fragment">Accuracy ≈ .96, macro F1 ≈ .93, Kappa ≈ .85.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/lora.png" style="width:100.0%"> <span class="citation">Hu et al.&nbsp;(2021).</span></p>
</div></div>
</section>
<section id="multimodal-encoders-and-lmms-4" class="slide level2">
<h2>Multimodal Encoders and LMMs</h2>
<h3 id="high-level-behavioral-constructs">High-Level Behavioral Constructs</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">LMM-based <strong>user activity</strong> and <strong>intention measurement</strong>:
<ul>
<li class="fragment">Combine <strong>commercial solutions</strong> with prompt engineering.<br>
</li>
<li class="fragment">Measure high-level behavioral constructs (e.g., consuming media, functional activities).<br>
</li>
<li class="fragment">In one application, Krippendorff’s Alpha = 0.877, precision = 0.92, recall = 0.91.<br>
</li>
<li class="fragment">Context-based inference: identifying <strong>user intention</strong> behind each screen.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/intent.png" style="width:100.0%"> <span class="citation">Chang et al.&nbsp;(under review).</span></p>
</div></div>
</section>
<section id="pitfalls-and-future-directions" class="slide level2">
<h2>Pitfalls and Future Directions</h2>
<h3 id="challenges-limitations-future-possibilities">Challenges, Limitations, &amp; Future Possibilities</h3>
<ul>
<li class="fragment">Over-reliance on models can lead to <strong>subtle biases</strong> or overconfidence in results.<br>
</li>
<li class="fragment">Need for <strong>interpretability tools</strong> to better understand model decisions.<br>
</li>
<li class="fragment">Future expansions:
<ul>
<li class="fragment">Efficient model training and inference,<br>
</li>
<li class="fragment">Real-time analytics,<br>
</li>
<li class="fragment">User feedback and participatory design.<br>
</li>
</ul></li>
<li class="fragment">Importance of <strong>interdisciplinary collaborations</strong> for domain-specific content analysis.</li>
</ul>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="some-related-ideas" class="title-slide slide level1 center">
<h1>Some related ideas…</h1>

</section>
<section id="section" class="slide level2" data-background-video="vids/jh.mp4" data-background-size="100%">
<h2></h2>
</section>
<section id="the-language-of-biology" class="slide level2">
<h2>The Language of Biology</h2>
<!-- ![](vids/jh.mp4){.r-stretch} -->
<h3 id="digital-biology-biology-becomes-computable">Digital Biology = Biology Becomes Computable</h3>
<ul>
<li class="fragment">Integration of heterogeneous biological data (genes → cells → organs → behavior)</li>
<li class="fragment">Multi-scale modeling that links molecular processes to whole-organism function</li>
<li class="fragment">Networked, collaborative science powered by AI and high-performance computing</li>
<li class="fragment">Simulation + experimentation loop accelerates biological discovery</li>
<li class="fragment">Biology shifts from descriptive to predictive and engineering-compatible</li>
</ul>
</section>
<section id="the-language-of-biology-1" class="slide level2">
<h2>The Language of Biology</h2>
<h3 id="ai-as-the-engine-of-digital-biology">AI as the Engine of Digital Biology</h3>
<ul>
<li class="fragment">Foundation models enable pattern discovery in massive biological datasets</li>
<li class="fragment">Generative models simulate biological systems and guide interventions</li>
<li class="fragment">AI accelerates modeling from molecules → cells → circuits → behavior</li>
<li class="fragment">Computational frameworks unify biological processes across scales</li>
<li class="fragment">AI turns biology into something legible, learnable, and designable</li>
</ul>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="from-digital-trace-data-to-communication-styles" class="title-slide slide level1 center">
<h1>From Digital Trace Data to Communication Styles</h1>

</section>
<section id="a-framework-for-analyzing-and-linking-multimodal-social-media-content" class="slide level2">
<h2>A Framework for Analyzing and Linking Multimodal Social Media Content</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">MJ Cho, Chingching Chang, Yuan Hsiao, Hen-Hsen Huang<br>
</li>
<li class="fragment">RCHSS, Academia Sinica<br>
</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/as_scene.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="the-challenge-a-new-era-for-media-effects-research" class="slide level2">
<h2>The Challenge: A New Era for Media Effects Research</h2>
<h3 id="why-we-need-new-approaches">Why We Need New Approaches</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">The field is shifting from <strong>quantity of media use</strong> to <strong>content and its effects</strong> (Pouwels et al., 2024).<br>
</li>
<li class="fragment"><strong>Nature Research Intelligence</strong>: Multimodal communication as a frontier.<br>
</li>
<li class="fragment">Traditional methods are insufficient for <strong>personalized</strong> and <strong>fragmented</strong> media environments (Ohme et al., 2024; Otto et al., 2024).<br>
</li>
<li class="fragment">The <strong>video problem</strong>:
<ul>
<li class="fragment">Dominant form of social media content.<br>
</li>
<li class="fragment">Multimodal and challenging to study (Kroon et al., 2024).<br>
</li>
<li class="fragment">Audio channel remains an <strong>under-studied dimension</strong> of communication.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/media_effects.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="multimodality-and-media-psychology" class="slide level2">
<h2>Multimodality and Media Psychology</h2>
<video data-src="vids/lake.mp4" class="r-stretch" controls=""><a href="vids/lake.mp4">Video</a></video>
</section>
<section id="multimodality-and-media-psychology-1" class="slide level2">
<h2>Multimodality and Media Psychology</h2>
<h3 id="why-multimodal-thinking-matters">Why Multimodal Thinking Matters</h3>
<ul>
<li class="fragment">Online media and platform–operator (PO) data are inherently <strong>multimodal</strong>.<br>
</li>
<li class="fragment"><strong>Format / schema</strong> shape psychological effects.<br>
</li>
<li class="fragment">Media theory is also multimodal:
<ul>
<li class="fragment">visual framing,<br>
</li>
<li class="fragment">vocal tone,<br>
</li>
<li class="fragment">textual content.<br>
</li>
</ul></li>
<li class="fragment">Psychological effects of <strong>expression and tone</strong>.<br>
</li>
<li class="fragment">Social meaning of <strong>objects and scenes</strong>.<br>
</li>
<li class="fragment">Our goal:
<ul>
<li class="fragment">Measurement of <strong>styles</strong>,<br>
</li>
<li class="fragment">Their <strong>effects</strong>,<br>
</li>
<li class="fragment">A coherent analytical framework.</li>
</ul></li>
</ul>
</section>
<section id="a-roadmap-from-the-literature-our-contribution" class="slide level2 scrollable">
<h2>A Roadmap from the Literature &amp; Our Contribution</h2>
<h3 id="three-step-approach-pouwels-et-al.-2024">Three-Step Approach (Pouwels et al., 2024)</h3>
<ol type="1">
<li class="fragment"><strong>Collect digital trace data</strong> (DTD, e.g., via APIs, tracking).<br>
</li>
<li class="fragment">Perform <strong>automated content analysis</strong> (text and visuals).<br>
</li>
<li class="fragment">Conduct <strong>linkage analysis</strong> to study effects on outcomes.</li>
</ol>
<h3 id="our-contribution">Our Contribution</h3>
<ul>
<li class="fragment">An <strong>end-to-end framework</strong> that operationalizes this approach for <strong>video</strong>.<br>
</li>
<li class="fragment">Goes beyond isolated text or image analysis.<br>
</li>
<li class="fragment">Incorporates the crucial <strong>audio modality</strong>.</li>
</ul>
</section>
<section id="our-framework-an-overview" class="slide level2 scrollable">
<h2>Our Framework: An Overview</h2>
<h3 id="from-raw-videos-to-communication-styles">From Raw Videos to Communication Styles</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><p><strong>Goal</strong>: A replicable pipeline to transform raw videos into theoretically meaningful <strong>communication styles</strong>.</p></li>
<li class="fragment"><p><strong>Data</strong>:</p>
<ul>
<li class="fragment">398 Instagram videos from 12 U.S. Senate candidates during the 2024 election.</li>
</ul></li>
<li class="fragment"><p><strong>Three core stages</strong>:</p>
<ol type="1">
<li class="fragment"><strong>Preprocessing</strong> – Raw video files → analysis-ready data streams.<br>
</li>
<li class="fragment"><strong>Multimodal Feature Extraction</strong> – Cleaned data streams → comprehensive behavioral features.<br>
</li>
<li class="fragment"><strong>Style Identification &amp; Linking</strong> – Feature sets → interpretable styles for linkage analysis.</li>
</ol></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/multi_framework.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="stage-1-from-raw-video-to-analysis-ready-streams" class="slide level2">
<h2>Stage 1: From Raw Video to Analysis-Ready Streams</h2>
<h3 id="preprocessing-and-filtering">Preprocessing and Filtering</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><p><strong>Goal</strong>: Preprocess and transform noisy social media video into <strong>isolated communication sources</strong>.</p></li>
<li class="fragment"><p><strong>Visual filtering</strong>:</p>
<ul>
<li class="fragment">Isolate candidate presence using <strong>face detection</strong> (MediaPipe) and <strong>recognition</strong> (DeepFace).</li>
</ul></li>
<li class="fragment"><p><strong>Audio filtering</strong>:</p>
<ul>
<li class="fragment">Isolate candidate’s voice using <strong>denoising</strong> (Demucs) and <strong>speaker diarization</strong> (PyAnnote).</li>
</ul></li>
<li class="fragment"><p><strong>Text generation</strong>:</p>
<ul>
<li class="fragment">Transcribe only the filtered candidate audio with <strong>ASR (Whisper)</strong>.</li>
</ul></li>
<li class="fragment"><p><strong>Result</strong>: A validated set of synchronized <strong>video, audio, and text</strong> data — only the <strong>target communicator</strong>.</p></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/preprocessing.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="stage-2-quantifying-sight-sound-and-speech" class="slide level2">
<h2>Stage 2: Quantifying Sight, Sound, and Speech</h2>
<h3 id="multimodal-feature-extraction">Multimodal Feature Extraction</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><strong>Visual features (nonverbal performance)</strong>:
<ul>
<li class="fragment">Facial Action Units (AUs) → emotional expression.<br>
</li>
<li class="fragment">Head pose → engagement cues.<br>
</li>
<li class="fragment">Valence &amp; arousal (EmoNet) → affective state.</li>
</ul></li>
<li class="fragment"><strong>Audio features (vocal performance)</strong>:
<ul>
<li class="fragment">Prosodic cues → pitch (F0), intensity (MFCC0), speech rate.<br>
</li>
<li class="fragment">Vocal emotion → valence &amp; arousal (wav2vec2).</li>
</ul></li>
<li class="fragment"><strong>Textual features (verbal content)</strong>:
<ul>
<li class="fragment">Topic modeling (BERTopic) → substantive themes.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/feature_extraction.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="stage-3-identifying-communication-styles" class="slide level2">
<h2>Stage 3: Identifying Communication Styles</h2>
<h3 id="hybrid-quantitativequalitative-method">Hybrid Quantitative–Qualitative Method</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><p><strong>Goal</strong>: Identify a meaningful typology of <strong>styles</strong> from thousands of features.</p></li>
<li class="fragment"><p><strong>Unsupervised clustering</strong> (K-Means / HDBSCAN):</p>
<ul>
<li class="fragment">Discover data-driven patterns across multimodal feature space.</li>
</ul></li>
<li class="fragment"><p><strong>LLM-assisted labeling</strong>:</p>
<ul>
<li class="fragment">Bridge quantitative patterns with <strong>qualitative meaning</strong>.<br>
</li>
<li class="fragment">Derive interpretable <strong>style labels</strong>.</li>
</ul></li>
<li class="fragment"><p>Example styles identified (audiovisual):</p>
<ul>
<li class="fragment">“Cheerful Energetic”<br>
</li>
<li class="fragment">“Stony Warm”<br>
</li>
<li class="fragment">“Concerned Articulate”<br>
</li>
<li class="fragment">“Responsive Rapid”<br>
</li>
<li class="fragment">“Negative Depressed”</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/style_identification.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="stage-3-tri-modality-communication-styles" class="slide level2">
<h2>Stage 3: Tri-Modality Communication Styles</h2>
<h3 id="styles-from-visual-audio-and-text-combined">Styles from Visual, Audio, and Text Combined</h3>
<!-- :::: {.columns}
::: {.column}
- **Tri-modality cluster styles** capture interplay between:  
  - Visual behavior,  
  - Vocal delivery,  
  - Verbal content.

Example cluster types:

- **Formal Low-Energy Neutral**:  
  - ↓ Clout, ↓ Certitude, ↓ Tone, ↓ MFCC0, ↓ AUs, ↑ Neutral emotion.  
  - Flat, factual delivery, avoids emotion or confrontation.

- **Casual Expressive Happy**:  
  - ↑ Pronouns, ↑ filler, ↑ MFCC0, ↑ AU06 (smile), ↑ Happiness, ↓ Negative tone.  
  - Friendly, warm; aims to build rapport.

- **Analytic Calm Neutral**:  
  - ↑ Analytic language, ↑ word count, ↓ arousal, ↑ neutral face and tone.  
  - Informative, calm delivery to establish expertise.

- **Emotive Dynamic Angry**:  
  - ↑ Risk language, ↑ anger (text/audio), ↑ MFCC0, ↑ AU04/20, high anger scores.  
  - Emotionally intense delivery, likely to provoke or mobilize.

- **Empathic Warm Positive**:  
  - ↑ social/family words, ↑ positive emotion, ↑ AU12 (smile), ↑ tone, ↓ fatigue.  
  - Builds trust via warmth and positivity.
:::
::: {.column}
[Figure placeholder: Table of tri-modal styles]
:::
:::: -->

<img data-src="imgs/tri_modal_styles.png" style="width:100.0%" class="r-stretch"></section>
<section id="validating-the-styles" class="slide level2">
<h2>Validating the Styles</h2>
<h3 id="distribution-across-parties">Distribution Across Parties</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><p><strong>Goal</strong>: Demonstrate the validity of computationally derived styles.</p></li>
<li class="fragment"><p><strong>Results</strong>: Styles systematically vary by <strong>political party</strong>.</p></li>
<li class="fragment"><p><strong>Visual styles</strong>:</p>
<ul>
<li class="fragment">Democrats: more likely to use “Cheerful” and “Excited” styles.<br>
</li>
<li class="fragment">Republicans: favor “Serious,” “Frowning,” and “Still” visual approaches.</li>
</ul></li>
<li class="fragment"><p><strong>Audio styles</strong>:</p>
<ul>
<li class="fragment">Democrats: more often use a “Rapid” vocal style.<br>
</li>
<li class="fragment">Republicans: more often use a “Somber” tone.</li>
</ul></li>
<li class="fragment"><p><strong>Audiovisual styles</strong>:</p>
<ul>
<li class="fragment">Democrats: “Cheerful Energetic”.<br>
</li>
<li class="fragment">Republicans: “Concerned Articulate” and “Negative Depressed”.</li>
</ul></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/party_styles.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="style-effectiveness-and-party-differences" class="slide level2">
<h2>Style Effectiveness and Party Differences</h2>
<h3 id="how-style-effectiveness-varies-by-party">How Style Effectiveness Varies by Party</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment"><p>The <strong>effectiveness</strong> of a communication style often depends on <strong>party</strong>.</p></li>
<li class="fragment"><p>Examples:</p>
<ul>
<li class="fragment">“Neutral Calm” audiovisual style generates <strong>high engagement</strong> for Republicans but <strong>very little</strong> for Democrats.<br>
</li>
<li class="fragment">Democrats benefit more from a “Cheerful Energetic” approach, which is less effective for Republicans.</li>
</ul></li>
<li class="fragment"><p>Implication: Style is not universally good or bad; its impact is <strong>conditional on identity and context</strong>.</p></li>
</ul>
</div><div class="column">
<p><img data-src="imgs/style_charts.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="contributions-implications" class="slide level2">
<h2>Contributions &amp; Implications</h2>
<h3 id="what-this-framework-enables">What This Framework Enables</h3>
<ul>
<li class="fragment"><p>Direct, practical response to the <strong>CMM special issue</strong> call for computational social media effects research.<br>
</p></li>
<li class="fragment"><p><strong>Advances step 1</strong>: Pipeline for creating valid, analysis-ready DTD streams from raw video.<br>
</p></li>
<li class="fragment"><p><strong>Innovates step 2</strong>: Multimodal method that integrates <strong>sight, sound, and speech</strong>.<br>
</p></li>
<li class="fragment"><p><strong>Enables step 3</strong>: Generates validated, predictive variables for <strong>theory-driven linkage analyses</strong>.</p></li>
<li class="fragment"><p>Opens new research avenues:</p>
<ul>
<li class="fragment">Move beyond <strong>what is said</strong> to quantify <strong>how it is said</strong>.<br>
</li>
<li class="fragment">Paves the way for new theories of <strong>multimodal political communication</strong> and media effects.</li>
</ul></li>
</ul>
</section>
<section id="why-all-these-matters" class="slide level2">
<h2>Why All These Matters?</h2>
<h3 id="a-tale-of-scientific-evolution">A tale of scientific evolution</h3>
<ul>
<li class="fragment">When Biology Becomes Computable, Mind Becomes Modelable
<ul>
<li class="fragment">Cognition emerges from biological information processing</li>
<li class="fragment">Multi-scale biological models create mechanistic foundations for psychological theory</li>
<li class="fragment">Integrated biological + digital-trace data enables predictive models of behavior</li>
<li class="fragment">Synthetic agents &amp; personas become valid testbeds for psychological mechanisms</li>
<li class="fragment">Psychology shifts from correlational to mechanistic and intervention-driven</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="網路輿情的人智互動即時輔助社會議題理解" class="title-slide slide level1 center">
<h1>網路輿情的人智互動：即時輔助社會議題理解</h1>

</section>
<section id="研究團隊與背景" class="slide level2">
<h2>研究團隊與背景</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">人社中心跨領域團隊：卓牧融、張卿卿、蕭遠、黃瀚萱、張永儒<br>
</li>
<li class="fragment">目標：提升社會議題理解、促進公共理性對話<br>
</li>
<li class="fragment">基礎：台灣規模最大的網路輿情資料庫</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/as_scene.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="社會爭議的根源理解不足" class="slide level2">
<h2>社會爭議的根源：理解不足</h2>
<h3 id="認知偏差-新媒介生態-理解鴻溝">認知偏差 + 新媒介生態 = 理解鴻溝</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">認知協調、選擇性暴露<br>
</li>
<li class="fragment">演算法濾泡與迴聲室效應持續存在<br>
</li>
<li class="fragment">新媒介型態造成資訊碎片化<br>
</li>
<li class="fragment">認知權威分散與不穩定 → 難以形成公共共識</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/debate.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="以人與-ai-的互動提升議題理解" class="slide level2">
<h2>以人與 AI 的互動提升議題理解</h2>
<h3 id="多模態輿情資料-rag-multi-agent-system">多模態輿情資料 × RAG × Multi-agent System</h3>
<div class="columns6040">
<div class="column">
<ul>
<li class="fragment">台灣最完整的網路輿情資料庫<br>
</li>
<li class="fragment">檢索增強生成（RAG）提升資訊可得性<br>
</li>
<li class="fragment">多代理人系統（multi-agent）協作產生深度理解<br>
</li>
<li class="fragment">利用互動的框架化來引導認知過程</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/interface.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="以人為本的多模態實證研究" class="slide level2">
<h2>以人為本的多模態實證研究</h2>
<h3 id="全方位方法從社會科學到人機互動">全方位方法：從社會科學到人機互動</h3>
<div class="columns6040">
<div class="column">
<ul>
<li class="fragment">多模態資料在社會科學中的應用迅速成長<br>
</li>
<li class="fragment">跨模態分析（文本、影像、社群動態）<br>
</li>
<li class="fragment">結合人智互動以提升議題理解深度</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/multimodal_research.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="multi-agent-系統架構" class="slide level2">
<h2>Multi-agent 系統架構</h2>
<h3 id="rag-向量資料庫-多代理協同">RAG × 向量資料庫 × 多代理協同</h3>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">多階段流程：Drafting → Critiquing → Refining<br>
</li>
<li class="fragment">向量資料庫（multilingual-e5 + ChromaDB）<br>
</li>
<li class="fragment">RAG Q&amp;A Agent、Summary Agent（Gemini）<br>
</li>
<li class="fragment">支援逐段精煉、立場抽取、影片摘要</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/multi_agent.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="人智互動引導觀點光譜與-qa" class="slide level2">
<h2>人智互動引導：觀點光譜與 QA</h2>
<video data-src="vids/rag.m4v" class="r-stretch" controls=""><a href="vids/rag.m4v">Video</a></video>
</section>
<section id="人智互動引導觀點光譜與-qa-1" class="slide level2">
<h2>人智互動引導：觀點光譜與 QA</h2>
<h3 id="幫助使用者理解多元觀點降低二元對立">幫助使用者理解多元觀點，降低二元對立</h3>
<div class="columns6040">
<div class="column">
<ul>
<li class="fragment">系統協助呈現多元立場與論點光譜<br>
</li>
<li class="fragment">引導式 QA 幫助理解議題的關鍵爭點<br>
</li>
<li class="fragment">促進更成熟、更細膩的公共討論能力</li>
</ul>
</div><div class="column">
<p><img data-src="imgs/qa.png" style="width:100.0%"></p>
</div></div>
</section>
<section id="why-do-all-these-matters" class="slide level2">
<h2>Why Do All These Matters?</h2>
<h3 id="a-tale-of-scientific-evolution-1">A tale of scientific evolution</h3>
<ul>
<li class="fragment">A New Phase for Social Science
<ul>
<li class="fragment">Behavior becomes measurable across biological, cognitive, and digital layers</li>
<li class="fragment">Emotion/thought contagion, media effects, social connectedness become model-ready</li>
<li class="fragment">Computational social science gains grounding in biological and cognitive models</li>
<li class="fragment">Interventions can be designed across biological, psychological, and social scales</li>
<li class="fragment">Social science enters its own “digital biology moment” — a designable science of behavior</li>
</ul></li>
</ul>
</section>
<section id="thank-you" class="slide level2">
<h2>Thank You</h2>
<h3 id="questions">Questions?</h3>
<div class="columns">
<div class="column">
<h4 id="mu-jung-mj-cho-卓牧融">Mu-Jung ’MJ’ Cho | 卓牧融</h4>
<p><span class="neon-red">mjcho@as.edu.tw</span></p>
</div><div class="column">
<p><img data-src="imgs/closing_visual.jpg" style="width:30.0%"></p>
</div></div>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>